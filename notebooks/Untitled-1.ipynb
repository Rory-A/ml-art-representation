{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c4a665",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "_HERE = Path(__file__).resolve().parent if \"__file__\" in globals() else Path.cwd()\n",
    "_ROOT = _HERE.parent  # project root\n",
    "DATA_DIR = _ROOT / \"data\" / \"paintings\"\n",
    "CSV_PATH = _ROOT / \"data\" / \"paintings.csv\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(f\"Loaded {len(df)} rows\")\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(DEVICE)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model.eval()\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return clip_processor(images=image, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "\n",
    "embeddings = []\n",
    "failed_files = []\n",
    "\n",
    "for filename in tqdm(df[\"filename\"], desc=\"Extracting CLIP embeddings\"):\n",
    "    try:\n",
    "        image_path = DATA_DIR / filename\n",
    "        pixel_values = preprocess_image(image_path).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedding = clip_model.get_image_features(pixel_values=pixel_values)\n",
    "            embedding = embedding.squeeze().cpu().numpy()\n",
    "            embeddings.append(embedding)\n",
    "    except Exception as e:\n",
    "        failed_files.append((filename, str(e)))\n",
    "        embeddings.append([0.0] * 512)  # Dummy vector if fail\n",
    "\n",
    "df[\"embedding\"] = embeddings\n",
    "\n",
    "embedding_matrix = pd.DataFrame(embeddings)\n",
    "reducer = umap.UMAP(random_state=42)\n",
    "embedding_2d = reducer.fit_transform(embedding_matrix)\n",
    "\n",
    "df[\"x\"] = embedding_2d[:, 0]\n",
    "df[\"y\"] = embedding_2d[:, 1]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df[\"year_norm\"] = scaler.fit_transform(df[[\"year\"]])\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(df[\"x\"], df[\"y\"], c=df[\"year_norm\"], cmap=\"plasma\", s=10)\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label(\"Year (normalized)\")\n",
    "plt.title(\"CLIP Embedding of Picasso Paintings (UMAP Projection)\")\n",
    "plt.xlabel(\"UMAP-1\")\n",
    "plt.ylabel(\"UMAP-2\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88d60b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
