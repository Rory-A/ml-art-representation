{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c4a665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os from pathlib \n",
    "import Path\n",
    "import pandas as pd\n",
    "import torch from torchvision \n",
    "import transformsfrom PIL \n",
    "import Image from tqdm \n",
    "import tqdm from sklearn.preprocessing \n",
    "import MinMaxScaler\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "_HERE = Path(__file__).resolve().parent if \"__file__\" in globals() else Path.cwd()\n",
    "_ROOT = _HERE.parent  # project root\n",
    "DATA_DIR = _ROOT / \"data\" / \"paintings\"\n",
    "CSV_PATH = _ROOT / \"data\" / \"paintings.csv\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(f\"Loaded {len(df)} rows\")\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(DEVICE)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return clip_processor(images=image, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "\n",
    "embeddings = []\n",
    "failed_files = []\n",
    "\n",
    "for filename in tqdm(df[\"filename\"], desc=\"Extracting CLIP embeddings\"):\n",
    "    try:\n",
    "        image_path = DATA_DIR / filename\n",
    "        pixel_values = preprocess_image(image_path).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedding = clip_model.get_image_features(pixel_values)\n",
    "            embedding = embedding.squeeze().cpu().numpy()\n",
    "            embeddings.append(embedding)\n",
    "    except Exception as e:\n",
    "        failed_files.append((filename, str(e)))\n",
    "        embeddings.append([0.0] * 512)  # Dummy vector if fail\n",
    "\n",
    "df[\"embedding\"] = embeddings\n",
    "\n",
    "embedding_matrix = pd.DataFrame(embeddings)\n",
    "reducer = umap.UMAP(random_state=42)\n",
    "embedding_2d = reducer.fit_transform(embedding_matrix)\n",
    "\n",
    "df[\"x\"] = embedding_2d[:, 0]\n",
    "df[\"y\"] = embedding_2d[:, 1]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df[\"year_norm\"] = scaler.fit_transform(df[[\"year\"]])\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(df[\"x\"], df[\"y\"], c=df[\"year_norm\"], cmap=\"plasma\", s=10)\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label(\"Year (normalized)\")\n",
    "plt.title(\"CLIP Embedding of Picasso Paintings (UMAP Projection)\")\n",
    "plt.xlabel(\"UMAP-1\")\n",
    "plt.ylabel(\"UMAP-2\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88d60b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
