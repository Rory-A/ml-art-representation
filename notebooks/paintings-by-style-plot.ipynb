{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paintings by Style: CLIP + UMAP Scatter\n\nThis notebook mirrors the Picasso embedding plot, but uses `data/paintings-by-style.csv`. Points are colored by `style`, and hover shows artist, title, and year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import umap\n",
    "import plotly.express as px\n",
    "from IPython.display import display\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except Exception:\n",
    "    tqdm = lambda x, **k: x\n",
    "\n",
    "_HERE = Path(__file__).resolve().parent if \"__file__\" in globals() else Path.cwd()\n",
    "_ROOT = _HERE.parent\n",
    "DATA_DIR = _ROOT / 'data' / 'paintings-by-style'\n",
    "CSV_PATH = _ROOT / 'data' / 'paintings-by-style.csv'\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "# Normalize column names for easier access\n",
    "df = df.rename(columns={\n",
    "    'directory+filename': 'rel_path',\n",
    "    'painting title': 'title',\n",
    "    'year of painting': 'year'\n",
    "})\n",
    "# Derive artist from rel_path: style/artist/filename\n",
    "def extract_artist(p: str) -> str:\n",
    "    parts = str(p).split('/')\n",
    "    return parts[1] if len(parts) >= 2 else ''\n",
    "\n",
    "df['artist'] = df['rel_path'].astype(str).map(extract_artist)\n",
    "df['image_path'] = df['rel_path'].apply(lambda p: str(DATA_DIR / p))\n",
    "df['year'] = pd.to_numeric(df['year'], errors='coerce')\n",
    "print(f'Loaded {len(df)} rows from {CSV_PATH.name}')\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute CLIP embeddings\n",
    "clip_model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32').to(DEVICE)\n",
    "clip_processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
    "clip_model.eval()\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    return clip_processor(images=image, return_tensors='pt')['pixel_values']\n",
    "\n",
    "embeddings = []\n",
    "failed = []\n",
    "for p in tqdm(df['image_path'], desc='Extracting CLIP embeddings'):\n",
    "    try:\n",
    "        pixel_values = preprocess_image(p).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            emb = clip_model.get_image_features(pixel_values=pixel_values)\n",
    "        embeddings.append(emb.squeeze().cpu().numpy())\n",
    "    except Exception as e:\n",
    "        failed.append((p, str(e)))\n",
    "        embeddings.append(np.zeros(512, dtype=np.float32))\n",
    "\n",
    "if failed:\n",
    "    print(f'Failed to embed {len(failed)} images (filled with zeros).')\n",
    "\n",
    "emb_matrix = np.vstack(embeddings)\n",
    "reducer = umap.UMAP(random_state=42)\n",
    "embedding_2d = reducer.fit_transform(emb_matrix)\n",
    "df['x'] = embedding_2d[:, 0]\n",
    "df['y'] = embedding_2d[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive scatter colored by style with informative hover\n",
    "# Enforce equal axis range for a square canvas\n",
    "xmin, xmax = df['x'].min(), df['x'].max()\n",
    "ymin, ymax = df['y'].min(), df['y'].max()\n",
    "xc, yc = (xmin + xmax) / 2.0, (ymin + ymax) / 2.0\n",
    "half = max(xmax - xmin, ymax - ymin) / 2.0\n",
    "half = 0.5 if half <= 0 else half\n",
    "\n",
    "fig = px.scatter(\n",
    "    df, x='x', y='y', color='style',\n",
    "    hover_name='title',\n",
    "    hover_data={'artist': True, 'year': True, 'style': False, 'rel_path': False},\n",
    "    width=900, height=900,\n",
    ")\n",
    "fig.update_layout(\n",
    "    title='CLIP Embedding of Paintings by Style (UMAP Projection)',\n",
    "    template='plotly_white',\n",
    ")\n",
    "fig.update_xaxes(range=[xc - half, xc + half], visible=True, constrain='domain')\n",
    "fig.update_yaxes(scaleanchor='x', scaleratio=1, visible=True, constrain='domain')\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

